#FOR DEBUG, examples
DebugFlags=NodeFeatures,NO_CONF_HASH,Power
ClusterName=local
AuthType=auth/munge
CryptoType=crypto/munge
MpiDefault=none
ProctrackType=proctrack/cgroup
ReturnToService=2
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdParameters=config_overrides
#vital for cloud on demand
SlurmctldParameters=enable_configless,idle_on_node_suspend
SlurmdSpoolDir=/var/spool/slurm/d
SlurmUser=root
StateSaveLocation=/var/spool/slurm/ctld
SwitchType=switch/none
TaskPlugin=task/none
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
Waittime=0
SchedulerType=sched/backfill
SelectType=select/linear
AccountingStoreJobComment=YES
JobCompType=jobcomp/filetxt
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=headnode1
AccountingStoragePort=6819
#AccountingStorageLoc=/var/log/slurm/accounting
AccountingStorageTRES=Mem,Node,CPU,Billing
JobCompLoc=/var/log/slurm/job_completions


SuspendProgram=/usr/lib/shc4hpc/handlers/suspend/node_suspend
ResumeProgram=/usr/lib/shc4hpc/handlers/resume/node_resume
ResumeFailProgram=/usr/lib/shc4hpc/handlers/recovery/node_recovery

#To reschedule jobs that are allocated to nodes that fail to resume (eg quotas)
## Make sure SlurmdTimeout (job requeue time aver resume sent) s is less than 
#SuspendTime (idle nodes marked for power saving) but (for rhobustness) greater than 
# boot times of slowest i.e. azure, nodes or for experimentation with over provisioning
# at least ResumeTimeout must be greater than boot times of slowest nodes
SlurmdTimeout=1800
#seems this takes an extra 25-35 percent of whats here
SuspendTime=3600
ResumeTimeout=1800
SuspendTimeout=1200
#testing failback operations speeded up for osk only
#SlurmdTimeout=30
#SuspendTime=50
#ResumeTimeout=120
#SuspendTimeout=30

TreeWidth=128
#SuspendExcNodes=headnode,metal1,trad1,trad2

SlurmctldLogFile=/var/log/slurmctld.log
SlurmdLogFile=/var/log/slurm.log
SlurmSchedLogFile=/var/log/slurmshed.log

#FOR DEBUG

#ResumeTimeout=60
#SlurmctldDebug=debug4
#DebugFlags=Power
#SlurmdDebug=3

#TODO, need to automate additoon of this line and make it for the master
#NodeName=localhost CPUs=2 State=UNKNOWN
#THINGS TO BE AWARE OF
#Quota set to 10 instances in openstack, be dsure that this matches
#Image size set to m1.small, be sure CPUS match here
#NodeName=kvm[1-2] CPUs=1 State=CLOUD Feature=CLOUD
#NodeName=osk[1-3] CPUs=1 State=CLOUD Feature=CLOUD
NodeName=azure[1-10] CPUs=1 State=CLOUD Feature=CLOUD
#NodeName=metal1 Sockets=2 CoresPerSocket=14 ThreadsPerCore=2 State=CLOUD Feature=CLOUD
#PartitionName=openstack Nodes=osk[1-3] Default=YES MaxTime=INFINITE State=UP
PartitionName=azure Nodes=azure[1-4] Default=NO MaxTime=INFINITE State=UP
#PartitionName=azures Nodes=azure[5-7] Default=NO MaxTime=INFINITE State=UP
#PartitionName=metal Nodes=metal1 Default=NO MaxTime=INFINITE State=UP
#PartitionName=kvm   Nodes=kvm[1-2]  Default=NO MaxTime=INFINITE State=UP




